<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于Emby搭建媒体服务器]]></title>
    <url>%2Fblog%2F2019%2F03%2F12%2Fbuilding-a-media-server-based-on-emby%2F</url>
    <content type="text"><![CDATA[最近，团队里在开展针对新员工的一系列培训，并将培训过程录制成了一个个的视频。可是分享培训视频却十分的麻烦，基于samba将文件夹共享给团队成员，然后想看培训视频的同学将视频拷贝到本地进行观看，这样实在是太浪费时间和空间。于是，我想到完全可以利用组里的服务器自己搭建一个媒体服务器，把URL共享出去，这样组里面的小伙伴就可以在线观看培训视频了。 0x01 选型有了这个想法，接下来就是去google上找有没有方便快捷的开源项目可以直接使用，寻找方向就是能开箱即用快速搭建，并且在docker hub上有现成的docker镜像。刚开始我看有人推荐了jellyfin，于是赶紧去docker hub Jellyfin查一查怎么使用，简单写了个docker-compose文件就将jellyfin起了起来。但在使用的过程中发现jellyfin的性能不太好，我通过浏览器访问时，页面要刷半天才能刷出来。我想着急性子的小伙伴们肯定忍受不了这样的慢网页呀，所以又接着尝试了Emby，简单测试了下，性能还不错，于是便选定了使用Emby来实现团队的媒体服务器。 0x02 快速搭建有了docker之后，要使用一个开源工具简直不要太方便，总共就三步： 在 docker hub 上找到这个工具的页面 按照使用说明写好docker-compose.yml文件 docker-compose up -d 将我的docker-compose.yml共享在这里，参考docker hub 上的embyserver 1234567891011version: "3"services: embyserver: image: emby/embyserver:latest ports: - 8096:8096 - 8920:8920 volumes: - /root/emby/config:/config - /root/emby/share1:/mnt/share1 - /root/emby/share2:/mnt/share2 起来之后，通过浏览器访问http://serverip:8096，经过一些简单的配置就可以使用起来了。 0x03 总结其实，Emby可以用来作为家庭的媒体服务器，通过NAS搭建一个服务器，然后使用它提供的各类接入App，就能多终端接入了。 看到这篇文章的小伙伴如果有类似的更好的开源工具或解决方案，欢迎分享给我。]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[仿写一个简单的RPC框架]]></title>
    <url>%2Fblog%2F2018%2F12%2F15%2Fwrite-a-tiny-rpc-framework%2F</url>
    <content type="text"><![CDATA[这两天在看《Netty实战》这本书，本着“纸上得来终觉浅，绝知此事要躬行”的态度，便决定学习使用Netty实现一些小东西以加深理解。在网上看了很多资料，遂决定仿造网上的一些教程用Netty实现一个简单的RPC，顺便也学习一下RPC的原理。源码见https://github.com/tomoyadeng/trpc 0x00 RPC要素以及技术选型RPC(远程过程调用)，简单来讲就是要实现：调用远程计算机上的服务，就像调用本地服务一样。 一个简单的RPC主要包含如下的几个要素： 注册中心(Registry): 提供服务注册与服务发现的功能 服务提供方-Provider(Server): 服务提供方 服务调用方-Consumer(Client): 服务调用方 一次RPC调用的流程如下图（忽略掉注册和发现的过程）： 根据RPC的技术需求，参考网上其他小伙伴的例子，最后决定使用下面一些技术: 技术关键点 说明 技术选型 选型说明 注册中心 要有注册中心来提供服务注册和发现功能 Etcd 主要考虑到部署Etcd比Zookeeper更容易 网络框架 实现远程调用需要将Client的请求发送到Server并接收调用结果 Netty 公认的高性能网络框架 序列化 编程时是面向对象的，传输是面向字节的，需要在网络传输时将对象和字节互相转化 Protostuff 基于 Protobuf 序列化框架，面向 POJO，无需编写 .proto 文件 动态代理 动态代理客户端使客户端不感知远程调用的过程 CGLib 强大的、高性能的代码生成库 接下来就看看如何实现各个层次的逻辑吧。 0x01 服务注册 &amp; 服务发现首先定义好注册中心的接口 12345public interface Registry &#123; void register(EndPoint endPoint, String serviceName) throws Exception; List&lt;EndPoint&gt; discovery(String serviceName) throws Exception;&#125; 12345678910@Data@AllArgsConstructorpublic class EndPoint &#123; private String host; private int port; public String toString() &#123; return host + ":" + port; &#125;&#125; 我这里采用Etcd作为注册中心，因此实现了一个EtcdRegistry 0x02 序列化/反序列化 &amp; Netty编码/解码RPC要把一次本地方法调用变成能够被网络传输的字节流，那么就需要考虑要进行序列化的对象是什么以及采用哪种序列化协议。 一次方法调用涉及到的对象实体就两个：请求和响应 12345678@Datapublic class TRpcRequest &#123; private long id; private String className; private String methodName; private Class&lt;?&gt;[] paramTypes; private Object[] params;&#125; 12345678910@Datapublic class TRpcResponse &#123; private long id; private Throwable exception; private Object result; public boolean hasException() &#123; return exception != null; &#125;&#125; 确定了需要序列化的对象实体，接下来就是确定序列化的协议，并实现序列化和反序列化方法。 高级的RPC框架一般都会抽象出序列化器/反序列化器的接口，并提供不同的实现，以供使用者进行选择。简单起见，我这儿直接写一个静态类来实现protobuf协议的序列化和反序列化。 123456789101112131415@UtilityClasspublic class SerializationUtil &#123; public static &lt;T&gt; byte[] serialize(T o) &#123; Schema schema = RuntimeSchema.getSchema(o.getClass()); return ProtostuffIOUtil.toByteArray(o, schema, LinkedBuffer.allocate()); &#125; public static &lt;T&gt; T desrialize(byte[] bytes, Class&lt;T&gt; clazz) &#123; Schema&lt;T&gt; schema = RuntimeSchema.createFrom(clazz); T message = schema.newMessage(); ProtostuffIOUtil.mergeFrom(bytes, message, schema); return message; &#125;&#125; 搞定了序列化对象以及协议，接下来就是实现Netty的编解码器来完成序列化和反序列化的逻辑，有了编解码器，在Netty中就可以Speak to POJO了。 Netty实现编码器和解码器比较简单，直接继承MessageToByteEncoder和ByteToMessageDecoder并实现其抽象方法即可。 12345678910111213141516public class TRpcEncoder extends MessageToByteEncoder &#123; private Class&lt;?&gt; targetClazz; public TRpcEncoder(Class&lt;?&gt; targetClazz) &#123; this.targetClazz = targetClazz; &#125; @Override protected void encode(ChannelHandlerContext ctx, Object msg, ByteBuf out) throws Exception &#123; if (targetClazz.isInstance(msg)) &#123; byte[] data = SerializationUtil.serialize(msg); out.writeInt(data.length); out.writeBytes(data); &#125; &#125;&#125; 123456789101112131415161718192021222324252627public class TRpcDecoder extends ByteToMessageDecoder &#123; private Class&lt;?&gt; targetClazz; public TRpcDecoder(Class&lt;?&gt; targetClazz) &#123; this.targetClazz = targetClazz; &#125; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; if (in.readableBytes() &lt; 4) &#123; return; &#125; in.markReaderIndex(); int dataLen = in.readInt(); if (dataLen &lt; 0) &#123; ctx.close(); &#125; if (in.readableBytes() &lt; dataLen) &#123; in.resetReaderIndex(); return; &#125; byte[] data = new byte[dataLen]; in.readBytes(data); Object obj = SerializationUtil.desrialize(data, targetClazz); out.add(obj); &#125;&#125; 0x03 Server端Server端需要完成如下几个任务： 处理调用：处理客户端的调用，完成对应的本地方法调用，并调用结果返回给客户端 监听端口：启动一个Server，并监听一个端口，以接收客户端的请求 服务注册：服务端提供哪些服务的调用，需要在服务启动时注册到注册中心上 接下来便来一步步实现上面任务的关键逻辑。 ServerHandlerNetty的处理流程是基于pipeline的，pipeline中包含一些完成具体操作的handler，前面已经实现的编码器和解码器也是handler，现在就要实现接收消息并完成调用的handler。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Slf4j@ChannelHandler.Sharablepublic class ServerHandler extends SimpleChannelInboundHandler&lt;TRpcRequest&gt; &#123; private final Map&lt;String, Object&gt; clazzMap; public ServerHandler(Map&lt;String, Object&gt; clazzMap) &#123; this.clazzMap = clazzMap; &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, TRpcRequest msg) throws Exception &#123; log.info("receive a request, id=&#123;&#125;", msg.getId()); TRpcResponse response = getResponse(msg); ctx.writeAndFlush(response).addListener((GenericFutureListener&lt;ChannelFuture&gt;) future -&gt; &#123; if (!future.isSuccess()) &#123; log.error("exception", future.cause()); &#125; &#125;); &#125; private TRpcResponse getResponse(TRpcRequest request) &#123; TRpcResponse response = new TRpcResponse(); response.setId(request.getId()); try &#123; Object target = clazzMap.get(request.getClassName()); if (target == null) &#123; throw new IllegalArgumentException(request.getClassName() + " not registered"); &#125; Class&lt;?&gt; clazz = Class.forName(request.getClassName()); FastClass fastClass = FastClass.create(clazz); FastMethod fastMethod = fastClass.getMethod(request.getMethodName(), request.getParamTypes()); Object result = fastMethod.invoke(target, request.getParams()); response.setResult(result); &#125; catch (Exception e) &#123; response.setException(e); log.error("invoke exception", e); &#125; return response; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; if (cause instanceof IOException) &#123; log.info("exception", cause); &#125; super.exceptionCaught(ctx, cause); &#125;&#125; 上面代码的逻辑就是：接收到Request后通过CGLib反射调用目标对象对应的方法，并将调用返回的结果包装成Response返回 ServerBootstrap接下来就要启动一个Server来监听一个端口，为了方便扩展，抽象一个Server接口，通过调用Server的start的方法来完成Server的启动，监听目标端口以响应客户端调用。 123public interface Server &#123; void start();&#125; 在Netty中是通过ServerBootstrap来引导一个Server，这里实现了一个SimpleServer来完成引导过程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Slf4jpublic class SimpleServer implements Server &#123; private EndPoint endPoint; private final Map&lt;String, Object&gt; clazzMap; public SimpleServer(EndPoint endPoint, Map&lt;String, Object&gt; clazzMap) &#123; this.endPoint = endPoint; this.clazzMap = clazzMap; &#125; public void start() &#123; EventLoopGroup boosGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(boosGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline() .addLast(new TRpcDecoder(TRpcRequest.class)) .addLast(new TRpcEncoder(TRpcResponse.class)) .addLast(new ServerHandler(clazzMap)); &#125; &#125;) .option(ChannelOption.SO_BACKLOG, 1024) .childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT) .childOption(ChannelOption.SO_KEEPALIVE, true) .childOption(ChannelOption.TCP_NODELAY, true); log.info("Server start at &#123;&#125;", endPoint); ChannelFuture future = bootstrap.bind(endPoint.getHost(), endPoint.getPort()).sync(); future.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; log.error("exception", e); &#125; finally &#123; workerGroup.shutdownGracefully(); boosGroup.shutdownGracefully(); &#125; &#125;&#125; 服务注册服务注册就是要将服务提供方所能提供的服务注册到注册中心上。为了使代码更小的侵入性，这里通过一个自定义的注解来标记要注册的服务实现其对应的接口： 123456@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documentedpublic @interface RpcService &#123; Class&lt;?&gt; value();&#125; 接下来实现在服务启动的时候就去扫描指定路径，完成服务注册。这里使用开源的Reflections来完成包扫描，并拿到所有带RpcService注解的实现类，解析出所有的服务名称后调用注册接口完成注册。 1234567891011121314151617Reflections reflections = new Reflections(packagePath);Set&lt;Class&lt;?&gt;&gt; classes = reflections.getTypesAnnotatedWith(RpcService.class);Map&lt;String, Object&gt; serviceMap = new ConcurrentHashMap&lt;&gt;();classes.forEach(clazz -&gt; &#123; RpcService rpcService = clazz.getAnnotation(RpcService.class); Class&lt;?&gt; rpcClazz = rpcService.value(); String clazzName = rpcClazz.getName(); try &#123; Object obj = clazz.getConstructor().newInstance(); serviceMap.put(clazzName, obj); registry.register(endPoint, clazzName); &#125; catch (Exception e) &#123; log.error("exception in register class " + clazzName, e); &#125;&#125;); 0x04 Client端Client 端要完成如下几个任务： 连接到服务端： 客服端需要连接到具体的服务端，才能进行远程调用(网络传输) 服务发现： 在调用远程服务时，需要到注册中心上查找具体有哪些服务端能提供对应的服务 动态代理： 需要动态生成代理对象来封装远程调用的网络过程 Handler首先要实现Handler来处理客服端发送请求和接收响应，由于Netty的handler是异步的，这里用简单的wait/notify来完成异步转同步 12345678910111213141516171819202122@Overrideprotected void channelRead0(ChannelHandlerContext ctx, TRpcResponse msg) throws Exception &#123; this.response = msg; log.info("receive rsp id=&#123;&#125;, result=&#123;&#125;", msg.getId(), msg.getResult()); synchronized (obj) &#123; obj.notifyAll(); &#125;&#125;public TRpcResponse send(TRpcRequest request) throws Exception &#123; if (channelFuture == null) &#123; connect(); &#125; channelFuture.channel().writeAndFlush(request).sync(); synchronized (obj) &#123; obj.wait(); &#125; return response;&#125; Bootstrap客户端的引导过程实际上就是连接到指定的远端地址 1234567891011121314151617public void connect() throws InterruptedException &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel channel) throws Exception &#123; channel.pipeline() .addLast(new TRpcEncoder(TRpcRequest.class)) .addLast(new TRpcDecoder(TRpcResponse.class)) .addLast(SimpleClient.this); &#125; &#125;) .option(ChannelOption.SO_KEEPALIVE, true); channelFuture = bootstrap.connect(endPoint.getHost(), endPoint.getPort()).sync();&#125; 服务发现服务发现就是要到注册中心上查找目标服务的服务提供方。为了使代码更小的侵入性，这里通过一个自定义的注解来标记要进行服务发现的接口。 12345@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documentedpublic @interface RpcApi &#123;&#125; 接下来就是实现在应用启动时，去扫描指定路径下的RPC接口，发现服务提供方的地址，将结果缓存下来并定时进行刷新。 12345678910111213141516171819202122232425 @Overridepublic void init() &#123; Reflections reflections = new Reflections(packagePath); Set&lt;Class&lt;?&gt;&gt; classes = reflections.getTypesAnnotatedWith(RpcApi.class); ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor(2); classes.forEach(clazz -&gt; executorService.scheduleAtFixedRate(() -&gt; &#123; try &#123; if (clazz.isInterface()) &#123; String className = clazz.getName(); List&lt;EndPoint&gt; endPoints = registry.discovery(className); log.info("discovery &#123;&#125; endPoints for &#123;&#125;", endPoints.size(), className); serviceMap.put(className, endPoints); endPoints.forEach(endPoint -&gt; &#123; if (clientPool.get(endPoint) == null) &#123; Client client = getClient(endPoint, this.group); clientPool.put(endPoint, client); &#125; &#125;); &#125; &#125; catch (Exception e) &#123; log.error("exception in register clazz " + clazz.getName(), e); &#125; &#125;, 0, 60, TimeUnit.SECONDS));&#125; 动态代理实现通过CGLib来实现客户端的动态代理，首先实现一个MethodInterceptor来完成如下任务： 构造request 获取client完成网络调用 解析response中的调用结果 12345678910111213141516171819202122232425262728public class ClientInterceptor implements MethodInterceptor &#123; private ClientFactory clientFactory; public ClientInterceptor(ClientFactory clientFactory) &#123; this.clientFactory = clientFactory; &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; TRpcRequest request = new TRpcRequest(); Class clazz = method.getDeclaringClass(); String clazzName = clazz.getName(); request.setId(System.currentTimeMillis()); request.setClassName(clazzName); request.setMethodName(method.getName()); request.setParamTypes(method.getParameterTypes()); request.setParams(args); Client client = clientFactory.getClient(clazzName); TRpcResponse response = client.send(request); if (response.hasException()) &#123; throw response.getException(); &#125; return response.getResult(); &#125;&#125; 随后提供一个代理工厂类来完成代理对象的创建 123456789@Slf4jpublic class ClientProxy &#123; public static &lt;T&gt; T create(Class&lt;T&gt; clazz, ClientFactory clientFactory) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(clazz); enhancer.setCallback(new ClientInterceptor(clientFactory)); return (T) enhancer.create(); &#125;&#125; 0x05 测试接下就是使用刚刚写好的RPC框架来实现一个简单的RPC调用流程 Etcd如何启动一个本地的Etcd作为注册中心这里就不赘述了，因为在Windows上启动一个Etcd比较简单，属于开箱即用的那种。 API首先定义好一个要进行RPC调用的接口，并将API打包成jar发布。 1234@RpcApipublic interface HelloService &#123; String hello(String hello);&#125; Server端实现新建一个 Spring boot 工程来实现Server端。 首先要添加API的依赖 12compile group: 'com.tomoyadeng', name: 'trpc-sample-api', version: '1.0'compile('org.springframework.boot:spring-boot-starter') 编写API的实现类 1234567891011121314151617@Slf4j@RpcService(value = HelloService.class)public class HelloServiceImpl implements HelloService &#123; @Override public String hello(String s) &#123; return "[" + getHostName() + "]: " + s; &#125; private String getHostName() &#123; try &#123; return Inet4Address.getLocalHost().toString(); &#125; catch (UnknownHostException e) &#123; return "localhost"; &#125; &#125;&#125; 编写启动类：启动类中包含了一些配置的处理 1234567891011121314151617181920212223242526272829303132333435public class ServerBootstrap &#123; @Value("$&#123;app.trpc.providerHost:#&#123;null&#125;&#125;") private String providerHost; @Value("$&#123;app.trpc.providerPort:#&#123;null&#125;&#125;") private String providerPort; @Value("$&#123;app.trpc.etcdRegistryAddr:#&#123;null&#125;&#125;") private String etcdResgitryAddress; @PostConstruct public void init() &#123; Configuration configuration = new Configuration(); if (providerHost != null) &#123; configuration.setProviderHost(providerHost); &#125; if (providerPort != null) &#123; configuration.setProviderPort(Integer.parseInt(providerPort)); &#125; Registry registry = etcdResgitryAddress == null ? new EtcdRegistry() : new EtcdRegistry(etcdResgitryAddress); start(configuration, registry); &#125; private void start(Configuration configuration, Registry registry) &#123; try &#123; ServerStarter starter = new ServerStarter(configuration, registry, "com.tomoyadeng.trpc.sample.server.impl"); Executors.newSingleThreadExecutor().execute(starter::start); &#125; catch (Exception e) &#123; log.error("exception", e); &#125; &#125;&#125; 启动应用： 123456@SpringBootApplicationpublic class TRpcSampleServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(TRpcSampleServerApplication.class, args); &#125;&#125; Client端实现新建一个 Spring boot 工程来实现客户端，并提供一个Rest调用的接口来触发RPC的调用。 首先还是添加 API的依赖，以及 Spring boot web 的依赖 12compile group: 'com.tomoyadeng', name: 'trpc-sample-api', version: '1.0'compile('org.springframework.boot:spring-boot-starter-web') 添加配置类，用以实现动态代理对象的自动装配 1234567891011121314151617181920212223@org.springframework.context.annotation.Configurationpublic class ClientConfiguration &#123; @Bean public ClientFactory clientFactory() &#123; Configuration configuration = new Configuration(); Registry registry = registry(configuration); ClientFactory clientFactory = new DefaultClientFactory(configuration, registry, "com.tomoyadeng.trpc.sample.api"); clientFactory.init(); return clientFactory; &#125; @Bean public Registry registry(Configuration configuration) &#123; return new EtcdRegistry(); &#125; @Bean @Scope(scopeName = "prototype") public HelloService helloService(ClientFactory clientFactory) &#123; return ClientProxy.create(HelloService.class, clientFactory); &#125;&#125; 提供一个RestController 123456789101112@RestController@RequestMapping(path = "/api/v1/trpcsample/hello")public class HelloController &#123; @Autowired private HelloService helloService; @GetMapping("") public String hello() &#123; return helloService.hello("hello"); &#125;&#125; 启动应用 123456@SpringBootApplicationpublic class TRpcSampleClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(TRpcSampleClientApplication.class, args); &#125;&#125; 启动OK后，直接通过浏览器访问http://localhost:8080/api/v1/trpcsample/hello进行测试 0x06 参考资料 徒手撸框架–实现 RPC 远程调用]]></content>
      <categories>
        <category>Middleware</category>
      </categories>
      <tags>
        <tag>rpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建 gitbook 的 docker 镜像]]></title>
    <url>%2Fblog%2F2018%2F12%2F08%2Fbuild-docker-image-for-gitbook%2F</url>
    <content type="text"><![CDATA[最近由于更换了VPS服务器，需要把原来老的服务器上发布的 gitbook 迁移到新的服务器上来。在新的服务器上重新安装 gitbook 的时候，遇到了各种恼人的npm/node版本的问题，遂决定构建一个 gitbook 的 docker 镜像，来避免后续在 gitbook 环境和版本问题上浪费时间。 0x00 构建 gitbook 的 docker 镜像构建一个docker镜像十分简单，总共就三步： 编写 Dockerfile build push 到 docker hub 编写 Dockerfilegitbook 的依赖很简单，使用 node 作为基础镜像，使用 npm 安装gitbook-cli 就行： 1234567891011121314151617FROM node:8.5-alpineMAINTAINER Tomoya &lt;tomoyadeng@gmail.com&gt;RUN npm install gitbook-cli -gARG GITBOOK_VERSION=3.2.3RUN gitbook fetch $GITBOOK_VERSIONENV BOOKDIR /gitbookVOLUME $BOOKDIREXPOSE 4000WORKDIR $BOOKDIRCMD ["gitbook", "--help"] 构建写好 Dockerfile 后， 直接运行 docker 命令行构建 1docker build . -t tomoyadeng/gitbook:latest 这里直接构建的镜像打上了tomoyadeng/gitbook:latest的标签，tomoyadeng 是我在Docker Hub 上注册好了的命名空间 push随后可以将构建好的镜像推送到 Docker Hub 123docker logindocker push tomoyadeng/gitbook:latest 使用1docker run --rm -v &quot;$PWD:/gitbook&quot; -p 4000:4000 tomoyadeng/gitbook gitbook &lt;command&gt; Frequently used gitbook commandsinstall 1docker run --rm -v &quot;$PWD:/gitbook&quot; -p 4000:4000 tomoyadeng/gitbook gitbook install build 1docker run --rm -v &quot;$PWD:/gitbook&quot; -p 4000:4000 tomoyadeng/gitbook gitbook build serve 1docker run --rm -v &quot;$PWD:/gitbook&quot; -p 4000:4000 tomoyadeng/gitbook gitbook serve more commands or details please refer to gitbook-cli 0x01 参考资料 gitbook-cli]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s上部署MySQL主从]]></title>
    <url>%2Fblog%2F2018%2F10%2F31%2Fk8s-mysql-cluster%2F</url>
    <content type="text"><![CDATA[K8s实验环境从minikube升级到单主多节点的集群后，接下来就准备把单实例的MySQL升级为MySQL主从架构。本文就介绍下怎么在K8s集群中部署MySQL主从，数据持久化采用nfs。 0x01 搭建 NFS 服务器参考 https://linuxconfig.org/how-to-configure-a-nfs-file-server-on-ubuntu-18-04-bionic-beaver 1apt install nfs-kernel-server 编辑 /etc/exports 1/nfs *(rw,sync,no_subtree_check,no_root_squash) 重启 1systemctl restart nfs-kernel-server 创建目录 12mkdir -p /nfs/mysql/data/mastermkdir -p /nfs/mysql/data/replica 0x02 准备 MySQL 主从镜像我们可以通过对MySQL官方的Dockerfile进行修改，然后构建生成主从镜像，首先把官方镜像克隆下来 1git clone https://github.com/docker-library/mysql.git 当然也可以直接把 Dockerfile 和 docker-entrypoint.sh 下载到本地(这里选择MySQL 5.7 版本) https://github.com/docker-library/mysql/tree/master/5.7 修改 master Dockerfile 将 Dockerfile 和 docker-entrypoint.sh 拷贝一份到一个新的目录，用于构建 master 镜像。 在 Dockerfile 中 VOLUME /var/lib/mysql 这一行前面加一行。这一行的作用是将mysql master的server-id设置为1。 1RUN sed -i '/\[mysqld\]/a server-id=1\nlog-bin' /etc/mysql/mysql.conf.d/mysqld.cnf 在docker-entrypoint.sh中添加如下内容，创建一个复制用户并赋权限，刷新系统权限表 123echo "CREATE USER '$MYSQL_REPLICATION_USER'@'%' IDENTIFIED BY '$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;" echo "GRANT REPLICATION SLAVE ON *.* TO '$MYSQL_REPLICATION_USER'@'%' IDENTIFIED BY '$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;" echo 'FLUSH PRIVILEGES ;' | "$&#123;mysql[@]&#125;" 修改 replica Dockerfile 将 Dockerfile 和 docker-entrypoint.sh 拷贝一份到一个新的目录，用于构建 replica 镜像。 在 Dockerfile 中 VOLUME /var/lib/mysql 这一行前面添加如下内容，将mysql replica的server-id设置为一个随机数: 1RUN RAND="$(date +%s | rev | cut -c 1-2)$(echo $&#123;RANDOM&#125;)" &amp;&amp; sed -i '/\[mysqld\]/a server-id='$RAND'\nlog-bin' /etc/mysql/mysql.conf.d/mysqld.cnf 在docker-entrypoint.sh中添加如下内容，配置连接master主机的host、user、password等参数，并启动复制进程。 123echo "STOP SLAVE;" | "$&#123;mysql[@]&#125;" echo "CHANGE MASTER TO master_host='$MYSQL_MASTER_SERVICE_HOST', master_user='$MYSQL_REPLICATION_USER', master_password='$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;" echo "START SLAVE;" | "$&#123;mysql[@]&#125;" 构建镜像1234567cd ~/dockerfile/mysql/master/docker build -t tomoyadeng/mysql-master:5.7.1docker push tomoyadeng/mysql-master:5.7.1cd ~/dockerfile/mysql/replica01docker build -t tomoyadeng/mysql-replica:5.7.1docker push tomoyadeng/mysql-replica:5.7.1 构建的时候可能会出错，重试几次就好了，docker-library/official-images#4252 (comment) 也可以选择直接使用我构建好的镜像 12docker pull tomoyadeng/mysql-master:5.7.1docker pull tomoyadeng/mysql-replica:5.7.1 0x03 创建 Secret新建 mysql-secret.yaml 12345678910apiVersion: v1kind: Secretmetadata: name: mysql-passtype: Opaquedata: rootuser: cm9vdA== rootpwd: MTIzNDU2 repluser: cmVwbA== replpwd: MTIzNDU2 创建 Secret 1kubectl create -f mysql-secret.yaml 0x04 创建 PV新建 master-pv.yaml 123456789101112131415161718apiVersion: v1kind: PersistentVolumemetadata: name: master-pvspec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: master mountOptions: - hard - nfsvers=4.1 nfs: path: /nfs/mysql/data/master server: 192.168.99.103 随后通过kubectl create -f master-pv.yaml 创建 master pv 新建 replica-pv.yaml 123456789101112131415161718apiVersion: v1kind: PersistentVolumemetadata: name: replica-pvspec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: replica mountOptions: - hard - nfsvers=4.1 nfs: path: /nfs/mysql/data/replica server: 192.168.99.103 随后通过kubectl create -f replica-pv.yaml 创建 replica pv 0x05 部署 master新建 mysql-master-deployment.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: v1kind: Servicemetadata: name: mysql-master labels: app: mysql-masterspec: ports: - port: 3306 targetPort: 3306 protocol: TCP selector: app: mysql-master---kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-master-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: master---apiVersion: apps/v1kind: Deploymentmetadata: name: mysql-master labels: app: mysql-masterspec: selector: matchLabels: app: mysql-master strategy: type: Recreate template: metadata: labels: app: mysql-master spec: containers: - image: tomoyadeng/mysql-master:5.7.1 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: rootpwd - name: MYSQL_REPLICATION_USER valueFrom: secretKeyRef: name: mysql-pass key: repluser - name: MYSQL_REPLICAITON_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: replpwd ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-master-pvc 上面的yaml文件包含了创建 master的 PVC，Deployment 和 Service，直接通过 kubectl create -f mysql-master-deployment.yaml 进行部署。 部署完成后，可以通过 kubectl get svc | grep master 查看刚才部署的 Service，然后可以通过运行MySQL客户端以连接到服务器 123456root@k8s-master001:~/k8s/mysql-cluster# kubectl run -it --rm --image=mysql:5.7 mysql-client -- mysql -h 10.97.247.244 -p123456kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.If you don't see a command prompt, try pressing enter.mysql&gt; 这里的ip是通过 kubectl get svc 查到的 CLUSTER-IP 0x06 部署 replica新建 mysql-replica-deployment.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374apiVersion: v1kind: Servicemetadata: name: mysql-replica labels: app: mysql-replicaspec: ports: - port: 3306 targetPort: 3306 protocol: TCP selector: app: mysql-replica---kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-replica-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: replica---apiVersion: apps/v1kind: Deploymentmetadata: name: mysql-replica labels: app: mysql-replicaspec: selector: matchLabels: app: mysql-replica strategy: type: Recreate template: metadata: labels: app: mysql-replica spec: containers: - image: tomoyadeng/mysql-replica:5.7.1 name: mysql env: - name: MYSQL_MASTER_SERVICE_HOST value: mysql-master - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: rootpwd - name: MYSQL_REPLICATION_USER valueFrom: secretKeyRef: name: mysql-pass key: repluser - name: MYSQL_REPLICAITON_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: replpwd ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-replica-storage mountPath: /var/lib/mysql volumes: - name: mysql-replica-storage persistentVolumeClaim: claimName: mysql-replica-pvc 这里的 MYSQL_MASTER_SERVICE_HOST 环境变量配置的就是之前master svc的名称。随后，直接使用 kubectl create -f mysql-replica-deployment.yaml 进行部署。过一会儿，可以通过运行客户端连接 replica，查看备份状态 0x07 总结至此，MySQL主从就搭建好了，工程源文件见 https://github.com/tomoyadeng/demo-springboot-k8s/tree/master/mysql-cluster]]></content>
      <categories>
        <category>Cloud Computing</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s折腾日记(零) -- 基于 Ubuntu 18.04 安装部署K8s集群]]></title>
    <url>%2Fblog%2F2018%2F10%2F12%2Fk8s-in-ubuntu18.04%2F</url>
    <content type="text"><![CDATA[基于 Kubernetes 1.12.1 之前折腾K8s的时候一直使用的是在 Ubuntu 虚拟机上起的 minikube，最近想在我的笔记本上使用多台虚拟机部署一套 Kubernetes 集群。正好今年上半年 Ubuntu 发布了新的LTS版本 – 18.04 (Bionic Beaver)，于是便有了这篇文章，在 Ubuntu 18.04 上折腾安装部署 K8s 集群。 0x00 前置条件 Ubuntu 18.04 虚拟机 Shadowsocks 科学上网 我的笔记本是Win10，通过 HyperV 安装了 3 台虚拟机，并对虚拟机进行如下规划： 主机名 ip 角色 配置 OS k8s-master001 192.168.99.103 master 2C2G Ubuntu 18.04 k8s-node001 192.168.99.104 node 1C2G Ubuntu 18.04 k8s-node002 192.168.99.105 node 1C2G Ubuntu 18.04 0x01 环境准备(所有虚拟机)Ubuntu虚拟机配置apt源国内配置apt源为阿里云的源下载速度会快一些，把 /etc/apt/sources.list 里面的内容替换成下面的内容即可 12345678910deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 更新 1apt-get update 配置 Shadowsocks 客户端进行科学上网1. 安装 Shadowsocks 客户端参考 Shadowsocks 设置方法 (Linux) 12apt-get install python-pippip install git+https://github.com/shadowsocks/shadowsocks.git@master 2. 创建 Shadowsocks 配置文件创建一个 /etc/shadowsocks.json 文件，格式如下 12345678910&#123; "server":"服务器 IP 或是域名", "server_port":端口号, "local_address": "127.0.0.1", "local_port":1080, "password":"密码", "timeout":300, "method":"加密方式 (chacha20-ietf-poly1305 / aes-256-cfb)", "fast_open": false&#125; 3. 启动 Shadowsocks1/usr/local/bin/sslocal -c /etc/shadowsocks.json -d start 4. 安裝 privoxy为了在终端中使用代理，还需要安装配置 privoxy (也可以选择 proxychains) 1apt-get install privoxy -y 5. 配置 privoxy安装完成后编辑 /etc/privoxy/config，搜索关键字 forward-socks5t，取消下面这一行的注释: 1forward-socks5t / 127.0.0.1:1080 . 这里 1080 对应着上面 Shadowsocks 配置文件中的“端口号”。 6. 启动 privoxy1systemctl start privoxy 7. 配置 proxy编辑 ~/.profile，并在文件末尾添加如下内容： 123456789export http_proxy=http://127.0.0.1:8118export https_proxy=http://127.0.0.1:8118export ftp_proxy=http://127.0.0.1:8118export no_proxy=localhost,127.0.0.0,127.0.1.1,127.0.1.1,10.0.0.0/8,192.168.0.0/16,mirrors.aliyun.comexport HTTP_PROXY=http://127.0.0.1:8118export HTTPS_PROXY=http://127.0.0.1:8118export FTP_PROXY=http://127.0.0.1:8118export NO_PROXY=localhost,127.0.0.0,127.0.1.1,127.0.1.1,10.0.0.0/8,192.168.0.0/16,mirrors.aliyun.com 执行 source ~/.profile 使之生效。 测试proxy是否可用 1curl www.google.com 关闭swap编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。参考Kubelet/Kubernetes should work with Swap Enabled 0x02 安装Docker(所有虚拟机)安装参考 安装Docker-阿里云 123456789101112# step 1: 安装必要的一些系统工具sudo apt-get updatesudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common# step 2: 安装GPG证书curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# Step 3: 写入软件源信息sudo add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"# Step 4: 更新并指定版本安装Docker-CE （这里安装 18.06.1~ce~3-0~ubuntu）sudo apt-get -y updateapt-cache madison docker-cesudo apt-get -y install docker-ce=18.06.1~ce~3-0~ubuntu 配置阿里云镜像加速参考 Docker 镜像加速器 配置加速地址，并重启 Docker 123456sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; "registry-mirrors": ["https://[阿里云分配的私有地址].mirror.aliyuncs.com"]&#125;EOF 配置 Docker 的 proxyKubernetes 的一些 docker 镜像是需要借助梯子才能拉取到的，为此需要为 Docker 配置 Proxy。 参考 https://docs.docker.com/config/daemon/systemd/#httphttps-proxy 配置文件 /etc/systemd/system/docker.service.d/http-proxy.conf 内容如下： 123[Service]Environment=&quot;HTTP_PROXY=http://127.0.0.1:8118/&quot;Environment=&quot;NO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,192.168.0.0/16,[阿里云分配的私有地址].mirror.aliyuncs.com&quot; 注意：这里的NO_PROXY需要上面配置的镜像加速地址添加进去 重启 Docker 12sudo systemctl daemon-reloadsudo systemctl restart docker 0x03 安装Kubernetes(所有虚拟机)参考 Installing kubeadm 1234567apt-get update &amp;&amp; apt-get install -y apt-transport-https curlcurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOFapt-get updateapt-get install -y kubelet kubeadm kubectl 注意： 目前kubernetes还没有 Ubuntu18.04 的编好的版本，用的 16.04 xenial 的二进制文件 0x04 配置master节点使用kubeadm init初始化master节点参考 https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 1kubeadm init --apiserver-advertise-address 192.168.99.103 --pod-network-cidr=10.244.0.0/16 init 常用主要参数： –kubernetes-version: 指定Kubenetes版本，如果不指定该参数，会从google网站下载最新的版本信息。 –pod-network-cidr: 指定pod网络的IP地址范围，它的值取决于你在下一步选择的哪个网络网络插件，比如我在本文中使用的是 flannel 网络，需要指定为10.244.0.0/16。 –apiserver-advertise-address: 指定master服务发布的Ip地址，如果不指定，则会自动检测网络接口，通常是内网IP。 kubeadm init 输出的token用于master和加入节点间的身份认证，token是机密的，需要保证它的安全，因为拥有此标记的人都可以随意向集群中添加节点。 安装网络插件安装一个网络插件是必须的，因为你的pods之间需要彼此通信。 网络部署必须是优先于任何应用的部署，详细的网络列表可参考插件页面。 本文使用的是flannel 网络，安装如下： 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 插件安装完成后，可以通过检查coredns pod的运行状态来判断网络插件是否正常运行。等待coredns pod的状态变成Running，就可以继续添加从节点了。 0x05 添加从节点在从节点上按照前面的步骤按照好docker和kubeadm后，就可以添加从节点到主节点上了 1kubeadm join 192.168.99.103:6443 --token k3c625.qjgzf8bdc8naufzt --discovery-token-ca-cert-hash sha256:3edad91bb59c6657e4b3dea984e9f484c56032b0d81d504e7e6e3615072f334b 过一会儿就可以通过 kubectl get nodes 命令在主节点上查询到从节点了 12345root@k8s-master001:~/k8s# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master001 Ready master 21h v1.12.1k8s-node001 Ready &lt;none&gt; 21h v1.12.1k8s-node002 Ready &lt;none&gt; 21h v1.12.1 0x06 安装 Dashboard 插件dashboard 支持两种模式： https 必须提供证书，参考 官方部署文档 kubectl apply -f https://github.com/kubernetes/dashboard/blob/master/src/deploy/recommended/kubernetes-dashboard.yaml http kubectl apply -f https://github.com/kubernetes/dashboard/blob/master/src/deploy/alternative/kubernetes-dashboard.yaml 我这里先把配置文件下到本地： 1wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/alternative/kubernetes-dashboard.yaml 并在配置文件最后添加 type: NodePort 以实现外网访问 12345678910111213141516# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: ports: - port: 80 targetPort: 9090 selector: k8s-app: kubernetes-dashboard type: NodePort 执行 kubectl create -f kubernetes-dashboard.yaml 安装。 为了简单，我这里直接为 Dashboard 赋予 Admin 的权限，参考https://github.com/kubernetes/dashboard/wiki/Access-control#admin-privileges 新增 dashboard-admin.yaml 并使用 kubectl create -f dashboard-admin.yaml 进行部署，文件内容如下： 1234567891011121314apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system 接下来就可以直接通过 nodeIp:port 访问 Dashboard 了。暴露的外部端口可以通过 kubectl get svc --namespace=kube-system 查看 1234root@k8s-master001:~/k8s# kubectl get svc --namespace=kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 21hkubernetes-dashboard NodePort 10.99.58.148 &lt;none&gt; 80:32094/TCP 20h 0x07 卸载集群想要撤销kubeadm做的事，首先要排除节点，并确保在关闭节点之前要清空节点。 在主节点上运行： 12kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;node name&gt; 然后在需要移除的节点上，重置kubeadm的安装状态： 1kubeadm reset]]></content>
      <categories>
        <category>Cloud Computing</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Stream API 简化代码]]></title>
    <url>%2Fblog%2F2018%2F10%2F07%2Fsimplify-code-with-stream-api%2F</url>
    <content type="text"><![CDATA[Stream API 是 Java 8 新引入的特性，用来增强集合操作。前段时间，在开发新需求以及重构老代码的时候，我开始尝试使用Java Stream API，使写出的代码更简洁也更易维护。这篇文章便对 Java Stream API 做一个总结，也记录一下我在使用过程中的一些心得和技巧。 0x01 从一个简单场景讲起先考虑这样一种可能在游戏代码里遇到的场景：每个玩家可以创建和培养众多游戏角色，现在从数据库或者其他地方获得了一批游戏角色信息，需要过滤出角色等级大于60的角色，并按照玩家ID进行归并。 这是一个常见的对数据进行过滤并归并的操作，如果在使用 JDK 7 的时候，我们的代码可能长这个样子： 123456789101112131415public Map&lt;Long, List&lt;Character&gt;&gt; groupCharacterByGamer() &#123; List&lt;Character&gt; characters = characterDao.getCharacters(); Map&lt;Long, List&lt;Character&gt;&gt; results = new HashMap&lt;&gt;(); for (Character character : characters) &#123; if (character.getLevel() &gt; 60) &#123; List&lt;Character&gt; gamerCharacters = results.get(character.getGamerId()); if (gamerCharacters == null) &#123; gamerCharacters = new ArrayList&lt;&gt;(); &#125; gamerCharacters.add(character); results.put(character.getGamerId(), gamerCharacters); &#125; &#125; return results;&#125; 这里列举下上面方法用到的实体类： 1234567891011121314151617@Datastatic class Gamer &#123; private Long id; private String name;&#125;@Datastatic class Character &#123; private Long id; private Long gamerId; private String name; private Integer level;&#125;public interface CharacterDao &#123; List&lt;Character&gt; getCharacters();&#125; 下面直接看下在 JDK 8 Stream API 的帮助下，我们可以怎样让代码简单一点点： 1234567public Map&lt;Long, List&lt;Character&gt;&gt; groupCharacterByGamerWithStreamAPI() &#123; List&lt;Character&gt; characters = characterDao.getCharacters(); return characters .stream() .filter(character -&gt; character.getLevel() &gt; 60) .collect(Collectors.groupingBy(Character::getGamerId));&#125; 先说直观感受，对比之前的版本，使用Stream API的代码变得简单和清晰了不少。 0x01 Lambda 表达式 &amp; 方法引用在讨论Stream API之前，先看看和 Stream API 一同被引入JDK 8 的另外两个特性： Lambda 表达式和方法引用。在上面的例子中c -&gt; c.getLevel() &gt; 60 就是一个 Lambda 表达式， 而 Character::getGamerId 则使用了方法引用，正是在这两个特性的加持下，Stream API 才变得异常强大。 初探 Lambda 表达式Java 通过引入 Lambda 表达式，为 Java 带来了函数式编程的手段，这样我们在参数传递时，不但能够传递对象，还能传递行为(函数)。在这之前，针对这种情况，一般是采用回调或者匿名内部类实现。比如，上面例子中的 c -&gt; c.getLevel() &gt; 60和下面冗长的代码实现的效果是一样的： 123456new Predicate&lt;Character&gt;() &#123; @Override public boolean test(Character character) &#123; return character.getLevel() &gt; 60; &#125;&#125; 关于为什么需要 Lambda 表达式的进一步讨论，可以看看这篇文章Why We Need Lambda Expressions in Java。 从上面的例子也看到了，Lambda 表达式以 (argument) -&gt; {body} 的形式呈现，但Lambda 表达式到底是什么呢？是新增的类型么？ 我在学习 Lambda 表示式时，总看到别人讲其背后是函数式接口，实际上，这两者之间的关系是： Java 使用函数式接口来表示 lambda 表达式类型，即，每个 Lambda 表达式都能隐式地赋值给函数式接口。例如，我们可以通过 Lambda 表达式创建 Runnable 接口的引用： 12Runnable r = () -&gt; System.out.print("Lambda");executor.submit(r); 实际上在使用的时候，通常不这样赋值后使用，而是直接使用下面的方式： 1executor.submit(() -&gt; System.out.print("Lambda")); 在未指定函数式接口类型时，编译器会根据方法的签名将对应的类型推断出来，上面例子中submit方法的签名为submit(Runnable task)，因此编译器会将该 Lambda 表达式赋值给 Runnable 接口。 谈谈函数式接口什么是函数式接口？–简而言之，就是只有一个抽象方法的接口，也被称为SAM(Single Abstract Method)接口。Java 8 之后，任何满足单一抽象方法法则的接口，都会被自动视为函数接口，所以 Runnable 和 Callable 接口也是函数式接口。值得注意的是：单一抽象方法并不代表接口只有一个方法，除了唯一的抽象方法外，函数式接口中可能还有接口默认方法或者静态方法。例如在之前的 Stream API 例子中使用的filter方法参数是一个Predicate&lt;T&gt;接口，其含义为：“接受一个输入参数，并返回一个布尔值结果”。除test方法为一个抽象方法外，Predicate 接口还有and，negate，or三个接口默认方法以及isEqual这个静态方法。 接口默认方法也是Java 8 新增的特性，这一改变使接口里可以不完全是抽象的内容，也可以添加特定的具体实现。例如经常在集合类上使用的forEach方法，它实际上来自于 Iterable 接口的一个接口默认方法： 123456default void forEach(Consumer&lt;? super T&gt; action) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125;&#125; 除了上面提到的Runable等传统函数式接口，以及Stream的filter使用的Predicate&lt;T&gt;，forEach 使用的Consumer&lt;T&gt;外，JDK 8 还包含多个新函数接口，比如 Supplier、BiConsumer&lt;T, U&gt; 和 BiFunction&lt;T, U, R&gt;，它们均是在java.util.function 包中定义的。 方法引用一言以蔽之，方法引用是用于简化Lambda表达式的一种手段。对于有些功能函数的实现已经存在的情况下，我们可以直接使用方法引用来构造Lambda表达式。为此，Java 使用了一个新的操作符::来表达方法引用。例如： 123List.of("a","b","c").forEach(s -&gt; System.out.println(s));// 等价于List.of("a","b","c").forEach(System.out::println); 方法引用通过类名和方法名来定位一个静态方法或者实例方法，其语法为ClassName::methodName 或者 instanceRefence::methodName，如果引用的方法是构造器，则方法名为new。方法引用比 Lambda 表达式更简洁，所以一般能直接改写成方法引用的方式就写成方法引用，且智能的IDE也会提示你进行简写。 0x02 Stream API 和 流式操作了解了 Lambda 表达式和方法引用，接下来就看看Stream API 是怎么和他们结合起来使用的。 首先需要明确的是，Stream 不是集合类，它本身是不保存数据的，它更像一个更高级的迭代器。不过同传统的迭代器不一样的是，它是内部迭代，不需要显示地把数据一个一个拿出来加工，只需要传入对元素的操作(函数)，Stream 便能够在内部完成迭代。 我们在使用流的时候，可以想象成一个流水线，数据在流水线上流动，会经过一系列的加工转换，最终生成我们想要的数据。流的操作一般可以看作有三个基本步骤：获取一个数据源 -&gt; 中间过程进行各种数据加工 -&gt; 执行终点操作以获取想要的结果。 例如： 1234567public Map&lt;Long, List&lt;Character&gt;&gt; groupCharacterByGamerWithStreamAPI() &#123; return characterDao .getCharacters() .stream() // 获取流数据源 .filter(c -&gt; c.getLevel() &gt; 60) // 中间操作 .collect(Collectors.groupingBy(Character::getGamerId)); // 终点操作&#125; 流的操作类型分为两种： 中间操作(Intermediate) 和 终点操作(Terminal)。一个流可以有N个中间操作，但是只能有一个终点操作。接下来就来看看怎么生成流，以及常见的流的中间操作和终点操作都有哪些。 创建 StreamJava 提供了多种方式来创建流，比较常用的有如下几种： 由集合或者数组直接生成: Collection.stream(), Arrays.stream(Object[]) 等 List.of(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;).stream() Arrays.stream(new String[] {&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}) 由流的静态方法生成： Stream.of(T... values), IntStream.range(int, int) 等 Stream.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) IntStream.range(1,10) 从文件中获得流：BufferedReader.lines(), Files.lines(Path path) 等 Files.newBufferedReader(Paths.get(&quot;/path/to/file&quot;),StandardCharsets.UTF_8).lines() Files.lines(Paths.get(&quot;/path/to/file&quot;)) 通过迭代或者生成器自己创建流： Stream.iterate(Object, UnaryOperator)，Stream.generate(Supplier&lt;? extends T&gt; s) 等 Stream.iterate(1, n -&gt; n + 2) Stream.generate(Math::random) 中间操作常用的流的中间操作主要有：map (mapToInt, flatMap 等)、 filter、 distinct、 sorted、 peek、 limit、 skip、unordered 等。 下面简单介绍下几个最常见的中间操作，剩下的可以查官方API limit/skiplimit 返回 Stream 的前面 n 个元素；skip 则是扔掉前 n 个元素。 123456// 生成5个随机数Stream.generate(Math::random).limit(5).forEach(System.out::println);// 跳过第一行的表头Files.lines(Paths.get("test.csv")).skip(1).forEach(System.out::println); filterfilter 对数据进行过滤，返回的流中只包含满足断言(predicate)的数据 12// 统计非空字符串数量long count = Stream.of("a", "b", "", "c").filter(s -&gt; !s.isEmpty()).count(); P.S. filter 里面的 s -&gt; !s.isEmpty() 是一个 Lambda 表达式，如果想换成方法引用的方式该怎么办呢？ 可以以一个静态方法为“管道”来进行转换，该静态方法以一个方法引用为输入，再以特定的函数接口为其返回，如 123public static &lt;T&gt; Predicate&lt;T&gt; as(Predicate&lt;T&gt; predicate) &#123; return predicate;&#125; 那么，上面的代码就可以改成这样了： 1long count = Stream.of("a", "b", "", "c").filter(as(String::isEmpty).negate()).count(); mapmap 方法将流中的元素映射为其他的值，新的值类型可以和原来的元素类型不同： 12// 将字符转换成 ASCII 码Stream.of('a', 'b', 'c').map(Object::hashCode).forEach(System.out::println); map 方法比较简单，但是使用的频率较高，这里我再举个栗子： 123456789101112131415/*** 从文件中按行读取字符串，并将字符串逐行转换成实体对象** @param path 文件路径* @return 转换后的实体类列表* @throws IOException 打开文件过程中的IO异常*/public List&lt;Entity&gt; parseToEntityList(Path path) throws IOException &#123; try (Stream&lt;String&gt; stream = Files.lines(path)) &#123; return stream.filter(as(String::isEmpty).negate()) .map(Entity::valueOf) // 省略 Entity 类定义 和 valueOf 方法 .collect(Collectors.toList()); &#125;&#125; 细心的同学应该注意到了，这个例子在创建和使用流的时候使用了 try-with-resources 的形式，这是为啥呢？一般来讲，流不需要手动去关闭，终点操作执行完之后，流就自动关闭了。但是，对于 Files.lines 这种会打开外部资源的操作，操作完之后需要手动关闭，从而确保资源正确关闭，不会引起内存泄漏。Files.lines 的注释也写明白了： This method must be used within a try-with-resources statement or similar control structure to ensure that the stream’s open file is closed promptly after the stream’s operations have completed. flatMapflatMap 方法结合了 map 和 flattern 的功能，它能将映射后的流的元素全部放入一个新的流中。该方法定义如下： 1&lt;R&gt; Stream&lt;R&gt; flatMap(Function&lt;? super T, ? extends Stream&lt;? extends R&gt;&gt; mapper); 从flatMap的参数来看，mapper 函数接受一个参数并返回一个 Stream，最后 flatMap 方法返回的流会包含所有 mapper 返回的流的元素。简单来讲，flatMap 会将流中的每一个元素(常见的是集合)都转换成一个流，并将这些流合并起来生成一个新的流，个人感觉有点 “降维” 的意味。看个例子吧： 123456 // 将多个List 合并成一个 ListList&lt;Integer&gt; numbers = Stream.of(List.of(1, 2), List.of(1, 2, 3), List.of(1, 2, 3, 4)) .flatMap(l -&gt; l.stream()) // 可以替换成更简洁的方法引用形式 flatMap(List::stream) .collect(Collectors.toList());System.out.println(numbers); // [1, 2, 1, 2, 3, 1, 2, 3, 4] 拆开来写成这样会不会清楚一些呢： 123456Function&lt;List&lt;Integer&gt;, Stream&lt;Integer&gt;&gt; mapper = List::stream;List&lt;Integer&gt; numbers = Stream.of(List.of(1, 2), List.of(1, 2, 3), List.of(1, 2, 3, 4)) .flatMap(mapper) .collect(Collectors.toList());System.out.println(numbers); // [1, 2, 1, 2, 3, 1, 2, 3, 4] 前面我们也提到了，方法引用是用于简化生成 Lambda 表达式的一种方式，而 Lambda 表达式都可以赋值给一个函数接口，这样是不是稍微清楚一些了，也就能清楚 flatMap 怎么使用了呢。 注： 无论是集合类，还是 Stream 都是使用的泛型接口，清楚的理解泛型及其限定关系(super, extends, ?)十分重要。 sorted对流进行排序可以通过 sorted 方法来实现，默认的sorted() 将流中的元素按照自然排序方式进行排序，也可以传入排序函数(Comparator接口)来指定排序的方式。例如： 123456// 获取系统变量，并按key的长度排序System.getenv() .keySet() .stream() .sorted((x, y) -&gt; x.length() - y.length()) // 还可简写为 .sorted(Comparator.comparingInt(String::length)) .forEach(System.out::println); 这比之前写匿名内部类的方式方便了不要太多，而且还可以通过先对流进行各种map、filter、distinct来减少元素数量后再排序，这样性能会好一点，且代码简洁清晰。 终点操作当终点操作执行后，流就无法再操作了，所以终点操作是流的最后一个操作。 forEachforEach 方法遍历流的每一个元素，执行指定的函数。前面的例子也多次用到了，比较简单，不再赘述。与其功能类似的一个中间操作是peek，peek 一般用于debug。 findFirst / findAny这两个操作都是终点操作兼短路操作(short-circuiting)，即可以不用遍历完所有元素就终止操作的： 12345Optional&lt;String&gt; result = Stream.of("one", "two", "three", "four") .peek(s -&gt; System.out.println("Iterated value: " + s)) .findFirst();System.out.println(result.orElse("Not found")); 这里值得注意的是，findFirst 和 findAny 返回的都是 Optional，可以将它理解为一个容器，它可能含有某个值，或者不包含。我们可以使用 Optional 来省去大量的丑陋的判空操作并有效的防止空指针异常。 回到一开始我们的例子，如果CharacterDao可能返回 null 的话 1234567public interface CharacterDao &#123; @Nullable List&lt;Character&gt; getCharacters(); @Nullable Character findOne(Long id);&#125; 在不使用 Optional 的情况下，可能会加上几行判空的代码： 1234567891011public Map&lt;Long, List&lt;Character&gt;&gt; groupCharacterByGamerWithStreamAPI() &#123; List&lt;Character&gt; characters = characterDao.getCharacters(); // 判空 if (characters == null) &#123; characters = Collections.emptyList(); &#125; return characters .stream() .filter(character -&gt; character.getLevel() &gt; 60) .collect(Collectors.groupingBy(Character::getGamerId));&#125; 再看下使用 Optional 的话，我们可以这样写： 12345678public Map&lt;Long, List&lt;Character&gt;&gt; groupCharacterByGamerWithStreamAPI() &#123; List&lt;Character&gt; characters = characterDao.getCharacters(); return Optional.ofNullable(characters) .orElse(Collections.emptyList()) .stream() .filter(character -&gt; character.getLevel() &gt; 60) .collect(Collectors.groupingBy(Character::getGamerId));&#125; 这样的话，看起来就简洁一些。再比如需要在返回结果为空的时候抛出异常，就可以这样写： 1234public Character findCharacterById(Long id) &#123; return Optional.ofNullable(characterDao.findOne(id)) .orElseThrow(() -&gt; new IllegalArgumentException(id + " not exists"));&#125; reducereduce 这个操作主要是把流中的元素组合起来生成一个值。它提供一个初始值(种子)，然后根据运算规则(BinaryOperator)，和前面Stream的第一个、第二个、第n个元素组合。通过reduce我们可以实现例如字符串拼接、数值求和等功能： 123456// 求和int sum = IntStream.range(1,5).reduce(0, (a,b) -&gt; a + b);// 字符串拼接String contact = Stream.of("a","b","c").reduce("", String::concat);// 字符串拼接，无种子，返回值为Optional，注意与上面的区别contact = Stream.of("a","b","c").reduce(String::concat).get(); collectcollect 在上面的例子也见过很多次了，这个操作是一个可变聚合(mutable reduction)操作，能将流中的元素累积到一个可变容器中。java.util.stream.Collectors这个辅助类来辅助进行各种reduction操作。 Collectors 主要包含了一些特定的收集器，如平均值averagingInt、最大最小值maxBy minBy、计数counting、分组groupingBy、字符串连接joining、分区partitioningBy、汇总summarizingInt、化简reducing、转换toXXX等。 下面举几个栗子： averagingInt 12// 求字符串长度平均值Double avg = Stream.of("abc", "bc", "c").collect(Collectors.averagingInt(String::length)); groupingBy 分组，在开头的例子里我们就使用到了分组，这里就不另举例子了。 partitioningBy 分区，其实是一种特殊的 groupingBy，它依照条件测试的是否两种结果来构造返回的数据结构。例如： 12345678// 按等级是否大于60分区public Map&lt;Boolean, List&lt;Character&gt;&gt; pationCharacterByLevel60() &#123; List&lt;Character&gt; characters = characterDao.getCharacters(); return Optional.ofNullable(characters) .orElse(Collections.emptyList()) .stream() .collect(Collectors.partitioningBy(character -&gt; character.getLevel() &gt; 60));&#125; 0xXX 参考资料 Java 8 中的 Streams API 详解 深入浅出 Java 8 Lambda 表达式]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Spring AOP & 自定义注解 记录日志]]></title>
    <url>%2Fblog%2F2018%2F09%2F24%2Flogging-with-aop-and-annotation%2F</url>
    <content type="text"><![CDATA[0x00 一切为了少写代码最近在重构一块儿业务代码的时候发现：有好几个类中有大量重复的代码，其实都在干同一件事，就是记录方法执行所耗时间，代码大概长这个样子： 1234567891011public void doSomething() &#123; long start = System.currentTimeMillis(); try &#123; // doSomeTimeConsumingTasks &#125; finally &#123; long cost = System.currentTimeMillis() - start; if (cost &gt; 1000) &#123; logger.info("doSomething cost &#123;&#125;s", cost / 1000.0); &#125; &#125;&#125; 数了一数，在一个方法里和具体业务并无关系的日志记录代码就占了8行。不对，在我们部门的code style里，左边的大括号是要换行的，代码应该长这个样子： 123456789101112131415public void doSomething() &#123; long start = System.currentTimeMillis(); try &#123; // doSomeTimeConsumingTasks &#125; finally &#123; long cost = System.currentTimeMillis() - start; if (cost &gt; 1000) &#123; logger.info("doSomething cost &#123;&#125;s", cost / 1000.0); &#125; &#125;&#125; 现在和具体业务并无关系的日志记录代码就变成12行了，如果多写几遍这样的方法，大概一个月的代码量KPI就够了。 不过，要做一个还算有点追求的程序猿，还是要力求在实现相同功能的前提下少写代码的，毕竟代码多了会让人没有读下去的欲望，不利于后期维护。 0x01 高举 Spring AOP 大旗使用过Spring AOP 的同学想必遇到这种重复日志的问题，早就计上心来：“给我一个切面，我能把你都记下来”。 最初的我也是这么想的，花了几分钟，定义了一个切面： 1234567891011121314151617181920212223242526272829303132333435363738394041package com.tomoyadeng.springbootutils.aop;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Component;import java.lang.reflect.Method;@Aspect@Componentpublic class LogAspect &#123; @Pointcut("execution(* com.tomoyadeng.springbootutils.service.*.*(..))") public void logPointCut() &#123;&#125; @Around("logPointCut()") public Object logTimeCost(ProceedingJoinPoint pjp) throws Exception &#123; MethodSignature signature = (MethodSignature) pjp.getSignature(); // 根据方法签名拿到方法的类，并通过类获取对应的logger Logger logger = LoggerFactory.getLogger(signature.getMethod().getDeclaringClass()); Method m = signature.getMethod(); long start = System.currentTimeMillis(); try &#123; return pjp.proceed(); &#125; catch (Throwable throwable) &#123; throw new Exception(throwable); &#125; finally &#123; long cost = System.currentTimeMillis() - start; // 记录日志 if (cost &gt; 1000) &#123; logger.info("&#123;&#125; cost &#123;&#125;s", m.getName(), cost / 1000.0); &#125; &#125; &#125;&#125; 写个测试： 1234567891011121314151617181920package com.tomoyadeng.springbootutils;import com.tomoyadeng.springbootutils.service.CustomizeService;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;@RunWith(SpringRunner.class)@SpringBootTestpublic class LogTimeCostTest &#123; @Autowired private CustomizeService service; @Test public void testLogForCostTime() &#123; service.someMethod(); &#125;&#125; 跑一下看看，果然能在日志中看到对应的日志 12018-09-24 00:58:24.963 INFO 6808 --- [ main] c.t.s.service.CustomizeService : someMethod cost 1.006s 日志记录算是基本完成了，不过仔细一想，这样会存在两个问题： 该包下并不是所有的类的公有方法都要记录耗时日志，有没有精确控制的办法呢？ 不是所有的方法记录耗时日志的阈值都是1000ms，有没有参数控制的办法呢？ 0x02 插上注解的翅膀回想我们在开发Spring应用时，经常会使用各种各样的注解来增强功能，比如@GetMapping， @ComponentScan等，那么上面提到的问题就可以通过注解来解决了。 首先，新建一个注解 1234567891011121314package com.tomoyadeng.springbootutils.annotation;import java.lang.annotation.*;@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface LogTimeCost &#123; // 自定义日志中写入的消息 String msg() default ""; // 自定义记录耗时日志的阈值 int threshold() default 0;&#125; 随后，对之前的切面进行改造： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.tomoyadeng.springbootutils.aop;import com.tomoyadeng.springbootutils.annotation.LogTimeCost;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Component;import java.lang.reflect.Method;@Aspect@Componentpublic class LogAspect &#123; @Pointcut("@annotation(com.tomoyadeng.springbootutils.annotation.LogTimeCost)") public void logPointCut() &#123;&#125; @Around("logPointCut()") public Object logTimeCost(ProceedingJoinPoint pjp) throws Exception &#123; MethodSignature signature = (MethodSignature) pjp.getSignature(); // 根据方法签名拿到方法的类，并通过类获取对应的logger Logger logger = LoggerFactory.getLogger(signature.getMethod().getDeclaringClass()); // 拿到函数上注解内的msg 和 threshold Method m = signature.getMethod(); String msg = ""; int threshold = 0; if (m.isAnnotationPresent(LogTimeCost.class)) &#123; LogTimeCost annotation = m.getAnnotation(LogTimeCost.class); msg = annotation.msg(); threshold = annotation.threshold(); &#125; long start = System.currentTimeMillis(); try &#123; return pjp.proceed(); &#125; catch (Throwable throwable) &#123; throw new Exception(throwable); &#125; finally &#123; long cost = System.currentTimeMillis() - start; // 记录日志 if (cost &gt; threshold) &#123; logger.info("&#123;&#125; [&#123;&#125;] cost &#123;&#125;s", m.getName(), msg, cost / 1000.0); &#125; &#125; &#125;&#125; 接下来，在需要记录耗时的方法前加上LogTimeCost注解即可 12345678@LogTimeCost(msg = "sleep 1 s", threshold = 500)public void someMethod() &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 最后，再跑一下测试 12018-09-24 01:20:55.267 INFO 11748 --- [ main] c.t.s.service.CustomizeService : someMethod [sleep 1 s] cost 1.007s 0x03 总结通过 Spring AOP 结合自定义注解，可以实现精确记录某些方法的耗时时间，这种方式简单方便，可以少写很多重复代码。但值得注意的是，这种方式还是存在一些限制的，这种方式只能用于public的方法上，这是由Spring AOP的实现原理决定的。可以参考官方文档的解释： 9.2.3 Declaring a pointcut#Supported Pointcut Designators Due to the proxy-based nature of Spring’s AOP framework, protected methods are by definition not intercepted, neither for JDK proxies (where this isn’t applicable) nor for CGLIB proxies (where this is technically possible but not recommendable for AOP purposes). As a consequence, any given pointcut will be matched against public methods only! If your interception needs include protected/private methods or even constructors, consider the use of Spring-driven native AspectJ weaving instead of Spring’s proxy-based AOP framework. This constitutes a different mode of AOP usage with different characteristics, so be sure to make yourself familiar with weaving first before making a decision.]]></content>
      <categories>
        <category>Framework</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s折腾日记(三)--为MySQL提供REST API]]></title>
    <url>%2Fblog%2F2018%2F09%2F23%2Fspringboot-k8s-3%2F</url>
    <content type="text"><![CDATA[在上一次的日志中，已经在 Kubernetes 中部署了一个单实例的 MySQL，接下来就是创建一个微服务来提供 MySQL 的 REST API， 让其他微服务能够通过 REST API 来访问数据库中的资源。 本项目源码 0x01 新建 Spring boot 服务在原有的父工程下新建一个 Module 命名为 k8s-db，新增如下的依赖 1234compile('org.springframework.boot:spring-boot-starter-web')compile("org.springframework.boot:spring-boot-starter-data-jpa")compile 'mysql:mysql-connector-java'compile group: 'javax.xml.bind', name: 'jaxb-api', version: '2.3.0' 注： k8s-db 是 demo-springboot-k8s 下的子工程，会继承在父工程引入的一些依赖包和定义的 task。jaxb-api 是在使用 JDK 9 以上的 JDK 版本时需要引入的依赖，因为它是 Java EE 里面的 module，在JDK 9 之后便从默认的包里面移除了，需要手动引入。 使用 Spring JPA 进行数据访问 创建一个 Application class 新建文件 com/tomoyadeng/demo/springboot/k8s/db/K8sDbApplication.java 1234567891011package com.tomoyadeng.demo.springboot.k8s.db;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class K8sDbApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(K8sDbApplication.class, args); &#125;&#125; 定义一个简单的 Entity 新建文件 com/tomoyadeng/demo/springboot/k8s/db/domain/Person.java 12345678910111213141516171819package com.tomoyadeng.demo.springboot.k8s.db.domain;import lombok.Data;import javax.persistence.Entity;import javax.persistence.GeneratedValue;import javax.persistence.GenerationType;import javax.persistence.Id;@Data@Entitypublic class Person &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private long id; private String firstName; private String lastName;&#125; 创建 JPA repository 提供简单查询 新建文件 com/tomoyadeng/demo/springboot/k8s/db/repository/PersonRepository.java 12345678910package com.tomoyadeng.demo.springboot.k8s.db.repository;import com.tomoyadeng.demo.springboot.k8s.db.domain.Person;import org.springframework.data.repository.CrudRepository;import java.util.List;public interface PersonRepository extends CrudRepository&lt;Person, Long&gt; &#123; List&lt;Person&gt; findByLastName(String lastName);&#125; 上面这几步操作是使用 Spring JPA 进行数据访问的步骤，参考 Accessing Data with JPA 创建 RestController新建文件 com/tomoyadeng/demo/springboot/k8s/db/controller/PersonController.java 1234567891011121314151617181920212223242526272829303132333435363738394041package com.tomoyadeng.demo.springboot.k8s.db.controller;import com.tomoyadeng.demo.springboot.k8s.db.domain.Person;import com.tomoyadeng.demo.springboot.k8s.db.repository.PersonRepository;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.util.Assert;import org.springframework.web.bind.annotation.*;import java.util.ArrayList;import java.util.List;@RestController@RequestMapping(path="/person")public class PersonController &#123; @Autowired private PersonRepository personRepository; @GetMapping("") public @ResponseBody List&lt;Person&gt; findAll() &#123; List&lt;Person&gt; results = new ArrayList&lt;&gt;(); personRepository.findAll().forEach(results::add); return results; &#125; @PostMapping("") public @ResponseBody Person add(@RequestBody Person person) &#123; return personRepository.save(person); &#125; @PutMapping("/&#123;id&#125;") public @ResponseBody Person update(@PathVariable("id") Long id, @RequestBody Person person) &#123; Assert.isTrue(id == person.getId(), "id not match"); return personRepository.save(person); &#125; @DeleteMapping("/&#123;id&#125;") public void delete(@PathVariable("id") Long id) &#123; personRepository.deleteById(id); &#125;&#125; 0x02 本地测试经过上面的步骤，一个使用 JPA 访问数据库的 Spring boot 的简单微服务就基本算是创建完了，下面需要在本地进行测试，看 REST API 能否正常访问。 使用 Sping profiles 来定义不同运行环境下的环境变量以及其他参数，我这里本地开发环境(Win10)定义为 dev，默认便使用 dev。 新建 application.yaml 123spring: profiles: active: dev 新建 application-dev.yaml 12345678910spring: profiles: dev jpa: hibernate: ddl-auto: update datasource: url: jdbc:mysql://localhost:3306/k8s?useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;useSSL=false driverClassName: com.mysql.jdbc.Driver username: root password: 123456 需要提前在本地安装好 MySQL， 并新建一个名为 k8s 的数据库。 Spring boot 应用启动后，URL： http://localhost:8080/person 能正常访问即可。 0x03 构建 Docker 镜像要使这个 Spring boot 应用跑在 Kubernetes 上，还是照例要先把它构建成 Docker 镜像。首先，要先新建一个 k8s 的 Spring profile。 新建文件： application-k8s.yaml 12345678910spring: profiles: k8s jpa: hibernate: ddl-auto: update datasource: url: jdbc:mysql://$&#123;MYSQL_HOST&#125;:$&#123;MYSQL_PORT&#125;/k8s?useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;useSSL=false driverClassName: com.mysql.jdbc.Driver username: $&#123;MYSQL_USER&#125; password: $&#123;MYQL_PASSWD&#125; MYSQL_HOST 等变量均需要在启动 docker 容器时注入。 随后便可以构建一个 docker 镜像了，我这里使用 gradle 来直接构建。 1gradle build buildDocker 0x04 部署 k8s-db 微服务到 Kubernetes新建 k8s 数据库在部署 k8s-db 微服务之前，需要先在 MySQL 中创建使用到的db，使用 kubectl exec 命令进入已经启动的容器的bash 1kubectl exec -it mysql-client-585479c646-nzl8m -- mysql -h mysql -p123456 1create database k8s; 这样，数据库就创建好了，接下来部署 k8s-db 微服务。 部署新建文件 k8s-db.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051apiVersion: v1kind: Servicemetadata: name: k8s-db labels: app: k8s-dbspec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: k8s-db---apiVersion: apps/v1kind: Deploymentmetadata: name: k8s-db labels: app: k8s-dbspec: replicas: 1 selector: matchLabels: app: k8s-db template: metadata: labels: app: k8s-db spec: containers: - name: k8s-db image: com.tomoyadeng/k8s-db:1.1 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 env: - name: MYSQL_HOST value: mysql - name: MYSQL_PORT value: '3306' - name: MYSQL_USER valueFrom: secretKeyRef: name: mysql-pass key: username - name: MYQL_PASSWD valueFrom: secretKeyRef: name: mysql-pass key: password 通过 kubectl create -f k8s-db.yaml 部署： 123root@k8s:~/k8s# kubectl create -f k8s-db.yamlservice/k8s-db createddeployment.apps/k8s-db created 随后便可以通过 kubectl get pod 看到新启动的 pod 了。 0x05 参考资料 Accessing Data with JPA 在 Kubernetes 上构建和部署 Java Spring Boot 微服务]]></content>
      <categories>
        <category>Cloud Computing</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s折腾日记(二)--在Kubernetes中部署一个单实例有状态的应用]]></title>
    <url>%2Fblog%2F2018%2F09%2F16%2Fspringboot-k8s-2%2F</url>
    <content type="text"><![CDATA[之前已经尝试在Kubernetes中部署了一个简单的Spring boot应用，但这个应用是无状态的，这次主要折腾一下如何运行一个单实例有状态应用(MySQL) 0x01 定义持久化磁盘Kubernetes 中使用 Volume 来持久化保存容器的数据，Volume 的生命周期独立于容器， Pod 中的容器可能频繁的被销毁和重建，但 Volume 会被保留。 从本质上来将， Kubernetes Volume 是一个目录，但 Volume 提供了对各种backend 的抽象，容器使用 Volume 时不用关心数据到底是存在本地节点的文件系统中还是类似 EBS 这种云硬盘中。 接下来，就通过 PersistentVolume 定义一个持久化磁盘： 为了方便，使用 hostPath 这种 Volume，这种类型是映射到主机上目录的卷，应该只用于测试目的或者单节点集群。 新建 local-volume.yaml 123456789101112apiVersion: v1kind: PersistentVolumemetadata: name: mysql-pvspec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce hostPath: path: /data/mysql 创建持久卷： 1kubectl create -f local-volume.yaml 随后就能通过 kubectl get pv 看到刚创建好的 PV 了。 0x02 创建 MySQL 密码 Secret在 Kubernetes 中可以通过 Secret 对象来保存类似于数据库用户名、密码或者密钥等敏感信息。Secret 会以密文的方式存储数据，避免了直接在配置文件中保存敏感信息，且 Secret 会以 Volume 的形式 mount 到 Pod，这样容器便可以使用这些数据了。 接下来通过yaml文件来创建数据库的 Secret，yaml文件中的敏感数据必须是通过 base64 编码后的结果。 1234root@k8s:~# echo -n root | base64cm9vdA==root@k8s:~# echo -n 123456 | base64MTIzNDU2 新建 mysql-secret.yaml 12345678apiVersion: v1kind: Secretmetadata: name: mysql-passtype: Opaquedata: username: cm9vdA== password: MTIzNDU2 创建 Secret 1kubectl create -f mysql-secret.yaml 随后便能通过 kubectl get secret 查看刚才创建的 Secret 0x03 部署 MySQLPersistentVolume (PV) 是外部存储系统的一块存储空间，在 Kubernetes 中，可通过 PersistentVolumeClaim (PVC) 来申请现已存在的 PV 的使用。 接下来通过yaml文件部署 MySQL： 新建 mysql-deployment.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162apiVersion: v1kind: Servicemetadata: name: mysql labels: app: mysqlspec: ports: - port: 3306 selector: app: mysql clusterIP: None---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-pv-claim labels: app: mysqlspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi---apiVersion: apps/v1kind: Deploymentmetadata: name: mysql labels: app: mysqlspec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim 部署 MySQL 1kubectl create -f mysql-deployment.yaml 随后便能通过 kubectl get pod 查看到部署的mysql了。 0x04 访问 MySQL 实例运行MySQL客户端以连接到服务器: 1kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h &lt;pod-ip&gt; -p&lt;password&gt; 这个命令在集群内创建一个新的Pod并运行MySQL客户端,并通过服务将其连接到服务器.如果连接成功,就知道有状态的MySQL database正处于运行状态. root@k8s:~/k8s# kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h mysql -p123456 If you don&apos;t see a command prompt, try pressing enter. mysql&gt; 至此，单实例的 MySQL 就在 Kubernetes 中部署成功了。 0x05 参考资料 运行一个单实例有状态应用 基于 Persistent Volumes 搭建 WordPress 和 MySQL 应用]]></content>
      <categories>
        <category>Cloud Computing</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s折腾日记(一)--在Kubernetes中部署spring boot应用]]></title>
    <url>%2Fblog%2F2018%2F09%2F13%2Fspringboot-k8s-1%2F</url>
    <content type="text"><![CDATA[基于容器的微服务架构目前已经成为了开发应用系统的主流，Kubernetes 则是运行微服务应用的理想平台。基于没事儿瞎折腾的态度，自己最近有空闲时间也开始鼓捣起了k8s，经过一步步摸索，终于完成安装和部署。接下来就先分享一下怎么在 kubernetes 中部署一个简单 Spring boot 的应用。 0x00 环境准备 minikube gradle minikube 的安装可以参考使用minikube安装k8s单节点集群 0x01 构建 Spring boot 应用首先，新建一个Spring Boot应用，姑且命名为k8s-service，这个应用就提供一个简单的接口，便于验证是否部署成功。 MainClass: 123456789101112package com.tomoyadeng.demo.springboot.k8s.service;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class K8sServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(K8sServiceApplication.class, args); &#125;&#125; RestController: 123456789101112package com.tomoyadeng.demo.springboot.k8s.service.controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class HelloController &#123; @GetMapping("/hello") public String hello() &#123; return "Hello, Kubernetes!"; &#125;&#125; 接下来写个测试用例，在本地跑一下： 123456789101112131415161718192021222324252627282930package com.tomoyadeng.demo.springboot.k8s.service;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.http.MediaType;import org.springframework.test.context.junit4.SpringRunner;import org.springframework.test.web.servlet.MockMvc;import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;import static org.hamcrest.Matchers.equalTo;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvcpublic class K8sServiceApplicationTest &#123; @Autowired private MockMvc mvc; @Test public void getHello() throws Exception &#123; mvc.perform(MockMvcRequestBuilders.get("/hello").accept(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()) .andExpect(content().string(equalTo("Hello, Kubernetes!"))); &#125;&#125; 完整代码参考Github 0x02 将应用打包成 Docker 镜像接下来就需要把 Spring boot 应用打包成 docker 镜像了，这样才能在 kubernetes 中运行。将 Spring boot 应用打包成docker镜像可以选择通过 Dockerfile 的方式，或者借助构建工具进行构建。我这里选择借助 gradle 来构建 docker 镜像(使用se.transmode.gradle:gradle-docker:1.2 插件)，在 build.gradle 定义构建docker镜像的任务： 123456789101112131415161718docker &#123; baseImage "openjdk:8-slim" maintainer 'tomoyadeng@gmail.com'&#125;task buildDocker(type: Docker, dependsOn: build) &#123; applicationName = bootJar.baseName tagVersion = bootJar.version doFirst &#123; copy &#123; from bootJar into stageDir &#125; &#125; volume "/tmp" addFile("$&#123;bootJar.baseName&#125;-$&#123;bootJar.version&#125;.jar", "app.jar") entryPoint(Arrays.asList("java", "-Djava.security.egd=file:/dev/./urandom", "-Dspring.profiles.active=k8s", "-jar", "/app.jar"))&#125; 然后执行 gradle build buildDocker 就能构建出docker镜像，构建完成后可以通过 docker images 查看。 0x03 将镜像部署到kubernetes中创建 DeploymentKubernetes Pod 是由一个或多个容器为了管理和联网的目的而绑定在一起构成的组。Deployment 是管理 Pod 创建和伸缩的推荐方法。要部署刚才构建好的容器镜像，首先要创建 Deployment。 新建 k8s-service.yaml 文件 12345678910111213141516171819202122apiVersion: apps/v1beta1kind: Deploymentmetadata: name: k8s-service labels: app: k8s-servicespec: replicas: 1 selector: matchLabels: app: k8s-service template: metadata: labels: app: k8s-service spec: containers: - name: k8s-service image: com.tomoyadeng/k8s-service:1.0 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 运行 kubectl create -f k8s-service.yaml 创建 Deployment，创建完成后可以通过 kubectl get deployments 查看 deployment，通过 kubectl get pods 查看 pod。 创建 Service默认情况下，Pod 只能通过 Kubernetes 集群中的内部 IP 地址访问。要使得 k8s-service 容器可以从 Kubernetes 虚拟网络的外部访问，您必须将 Pod 暴露为 Kubernetes Service。 1kubectl expose deployment k8s-service --type=LoadBalancer 随后可以通过 curl $(minikube service k8s-service --url)/hello 验证部署是否成功。部署的pod的日志可以通过 kubectl log &lt;Pod Name&gt; 来查看。 完整代码参考Github 0x04 参考资料 你好 Minikube]]></content>
      <categories>
        <category>Cloud Computing</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性哈希算法的一次实践]]></title>
    <url>%2Fblog%2F2018%2F06%2F09%2Fconsistent-hashing-practice%2F</url>
    <content type="text"><![CDATA[最近在分析一个需求，需要开发一个采集器的调度框架，实现采集器的注册，离线以及采集任务分配(负载均衡)。 采集器用于登录到网络设备上采集数据，部分运营商考虑到设备性能问题，会限制同时只能有一个用户登录设备查询数据。那么在此限制下，分配采集任务时需要保证： 对于同一设备的任务始终都落在同一个采集器上去执行，才能保证同一时刻对于同一设备不会有多个采集器采集。 而且，需要保证在某个采集器失效离线时，之前落在该采集器上的设备列表需要均匀分布到剩下的采集器上去，不至于造成某一个采集器负载过大。 到这里，实现方案已经呼之欲出，这不就是解决分布式缓存问题的套路么 — 一致性哈希算法，可以参考这篇文章进行了解 《一致性哈希算法及其在分布式系统中的应用》 0x01 接口定义首先，对几个关键角色进行接口抽象 网络设备12345678910/** * 网络设备 */public interface Device &#123; /** * 获取设备IP * @return IP */ String getIp();&#125; 采集器123456789101112131415161718192021public interface Collector &#123; /** * 获取采集器IP * @return IP */ String getIp(); /** * 设置采集器IP * @param ip IP */ void setIp(String ip); /** * 执行采集任务 * @param device 采集对象--设备 * @param commands 采集命令 * @return 采集结果 */ Map&lt;String, Object&gt; collect(Device device, List&lt;String&gt; commands);&#125; 集群这里把采集器的调度框架抽象成集群，并且使用泛型来定义集群接口 1234567891011121314151617181920212223/** * 调度框架(可看作集群管理) */public interface Cluster&lt;T&gt; &#123; /** * 注册 * @param t */ void register(T t); /** * 离线 * @param t */ void offline(T t); /** * 负载均衡 * @param ip 源IP * @return T */ T choose(String ip);&#125; 0x02 算法实现Hash算法选择在选择设备对应的采集器时，需要对设备的IP进行hash计算。由于设备的IP前缀基本一致，使用默认的字符串hash算法会导致计算出来的hash值不够离散，只能落在hash环上很小的一段区间。因此需要重新选择一种hash算法，保证字符串hash的离散性。这里使用FNV1_32_HASH算法 1234567891011121314151617181920212223/** * FNV1_32_HASH * * @param str str * @return hash */private static int rehash(String str) &#123; final int p = 16777619; int hash = (int) 2166136261L; for (int i = 0; i &lt; str.length(); i++) &#123; hash = (hash ^ str.charAt(i)) * p; &#125; hash += hash &lt;&lt; 13; hash ^= hash &gt;&gt; 7; hash += hash &lt;&lt; 3; hash ^= hash &gt;&gt; 17; hash += hash &lt;&lt; 5; if (hash &lt; 0) &#123; hash = Math.abs(hash); &#125; return hash;&#125; 具体实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** @author tomoya */public class CollectorCluster implements Cluster&lt;Collector&gt; &#123; /** 每个采集器定义虚拟节点个数 */ private static final int VIRTUAL_NODE_NUMBER = 320; /** 采集器--所有虚拟节点hash值数组 映射关系 */ @GuardedBy("clusterLock") private Map&lt;Collector, int[]&gt; registeredServers = new HashMap&lt;&gt;(); /** hash环上的hash值--采集器 映射关系 */ @GuardedBy("clusterLock") private TreeMap&lt;Integer, Collector&gt; hashRingMap = new TreeMap&lt;&gt;(); private ReadWriteLock clusterLock = new ReentrantReadWriteLock(); @Override public void register(Collector collector) &#123; System.out.println("add server " + collector.toString()); final Lock lock = clusterLock.writeLock(); lock.lock(); try &#123; // 计算采集器所有虚拟节点的hash值，并将所有hash值注册到hash环上 int[] nodesHash = new int[VIRTUAL_NODE_NUMBER]; for (int i = 0; i &lt; VIRTUAL_NODE_NUMBER; i++) &#123; int hash = CollectorCluster.rehash(collector.getIp() + ":" + i); nodesHash[i] = hash; hashRingMap.put(hash, collector); &#125; // 保存采集所有虚拟节点的hash值 registeredServers.put(collector, nodesHash); &#125; finally &#123; lock.unlock(); &#125; &#125; @Override public void offline(Collector collector) &#123; System.out.println("delete server " + collector.toString()); final Lock lock = clusterLock.writeLock(); lock.lock(); try &#123; // 将该采集器所有虚拟节点的hash值从hash环上删除 for (int hash : registeredServers.get(collector)) &#123; hashRingMap.remove(hash); &#125; // 删除采集器 registeredServers.remove(collector); &#125; finally &#123; lock.unlock(); &#125; &#125; @Override public Collector choose(String deviceIp) &#123; final int hash = rehash(deviceIp); final Lock lock = clusterLock.readLock(); lock.lock(); try &#123; // 逆时针找映射节点 Map.Entry&lt;Integer, Collector&gt; entry = hashRingMap.floorEntry(hash); Collector collector = entry == null ? hashRingMap.lastEntry().getValue() : entry.getValue(); System.out.println(deviceIp + " --&gt; " + collector); return collector; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 0x03 测试首先实现一个具体的采集器类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @author tomoya */public class DefaultCollector implements Collector &#123; private String ip; DefaultCollector(String ip) &#123; this.ip = ip; &#125; @Override public String getIp() &#123; return ip; &#125; @Override public void setIp(String ip) &#123; this.ip = ip; &#125; @Override public Map&lt;String, Object&gt; collect(Device ne, List&lt;String&gt; commands) &#123; return new HashMap&lt;&gt;(commands.size()); &#125; @Override public String toString() &#123; return "DefaultCollector&#123;" + "ip='" + ip + '\'' + '&#125;'; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; DefaultCollector that = (DefaultCollector) o; return Objects.equals(ip, that.ip); &#125; @Override public int hashCode() &#123; return CollectorCluster.rehash(ip); &#125;&#125; 然后直接在CollectorCluster类中增加一个main函数来测试 12345678910111213141516171819public static void main(String[] args) &#123; CollectorCluster cluster = new CollectorCluster(); // 注册5个采集器 List.of("192.168.0.1", "192.168.0.2", "192.168.0.3", "192.168.0.4", "192.168.0.5") .stream() .map(DefaultCollector::new) .forEach(cluster::register); String ipPrefix = "136.10.1."; // 20个设备 进行负载均衡 Stream.iterate(1, i -&gt; i + 1).limit(20).map(i -&gt; ipPrefix + i).forEach(cluster::choose); System.out.println("============"); // 离线一个采集器 cluster.offline(new DefaultCollector("192.168.0.5")); // 20个设备 再次进行负载均衡 Stream.iterate(1, i -&gt; i + 1).limit(20).map(i -&gt; ipPrefix + i).forEach(cluster::choose);&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker快速搭建Kafka开发环境]]></title>
    <url>%2Fblog%2F2018%2F06%2F02%2Fkafka-cluster-in-docker%2F</url>
    <content type="text"><![CDATA[最近准备学习Kafka，于是买了《Kafka权威指南》来看。作为一个初学者，快速搭建起一套可以运行的环境十分重要，跟着该书第2章的安装介绍可以完成在Linux系统下的环境搭建，但是读下来发现步骤还是有点繁多。有没有什么更加快捷的办法搭建一套可以运行的开发环境呢，于是我想到了Docker。2018年了，容器化已经成为了主流，在本地进行开发和测试的时候使用Docker也便于模拟多节点的集群环境。 0x00 前置条件 Docker: 要想使用Docker来启动kafka，开发环境提前装好Docker是必须的，我一般在Ubuntu虚拟机上进行开发测试 Docker Compose: kafka依赖zookeeper，使用docker-compose来管理容器依赖 0x01 Docker镜像要想使用Docker安装Kafka，第一件事当然是去Docker hub上找镜像以及使用方法啦。发现kafka并不像mysql或者redis那样有官方镜像，不过Google一下后发现可以选择知名的三方镜像wurstmeister/kafka wurstmeister/kafka在Github上更新还算频繁，目前使用kafka版本是1.1.0 0x02 安装 参考官方测试用的docker-compose.yml直接在自定义的目录位置新建docker-compose的配置文件 touch ~/docker/kafka/docker-compose.yml 123456789101112131415version: '2.1'services: zookeeper: image: wurstmeister/zookeeper ports: - "2181" kafka: image: wurstmeister/kafka ports: - "9092" environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.5.139 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 volumes: - /var/run/docker.sock:/var/run/docker.sock 注意： KAFKA_ADVERTISED_HOST_NAME 需要配置为宿主机的ip docker-compose 启动kafka 1root@ubuntu:~/docker/kafka# docker-compose up -d 启动完之后通过docker ps可以看到启动了一个zookeeper容器和一个kafka容器 启动多个kafka节点，比如3 1root@ubuntu:~/docker/kafka# docker-compose scale kafka=3 如果没什么错误的话，再通过docker ps可以看到启动了一个zookeeper容器和三个kafka容器 0x03 验证 首先进入到一个kafka容器中，例如: kafka_kafka_1 1root@ubuntu:~/docker/kafka# docker exec -it kafka_kafka_1 /bin/bash 创建一个topic并查看，需要指定zookeeper的容器名(这里是kafka_zookeeper_1)，topic的名字为test 123$KAFKA_HOME/bin/kafka-topics.sh --create --topic test --zookeeper kafka_zookeeper_1:2181 --replication-factor 1 --partitions 1$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper kafka_zookeeper_1:2181 发布消息，输入几条消息后，按^C退出发布 1$KAFKA_HOME/bin/kafka-console-producer.sh --topic=test --broker-list kafka_kafka_1:9092 接受消息 1$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server kafka_kafka_1:9092 --from-beginning --topic test 如果接收到了发布的消息，那么说明部署正常，可以正式使用了。]]></content>
      <categories>
        <category>Middleware</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 PriorityQueue 求解 Top K 问题]]></title>
    <url>%2Fblog%2F2018%2F05%2F13%2FtopK-with-PriorityQueue%2F</url>
    <content type="text"><![CDATA[0x00 问题描述这两天在看一些面试题的时候，遇到一个问题: 有N(N&gt;&gt;10000)个整数,求出其中的前K个最大的数 在网上搜了下，大概有三种解决思路： 排序：这种方式最好理解，但是时间复杂度较高(使用快排,O(NlogN)) 堆： 维护一个有边界的小顶堆(O(NlogK)) 位图： 理解较为困难 (O(N)) 自己动手试了试第二种思路在Java中的实现(泛型版本) 0x01 Java实现在 Java 中，PriorityQueue 类实现了堆这种数据结构，可以用来求解Top K 问题。 整个算法的思想就是： 通过PriorityQueue实现一个有界的堆，逐个向堆中添加元素，当元素个数超过边界时，淘汰堆顶元素 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.tomoyadeng.javabeginner.interview;import java.util.*;import java.util.stream.Stream;public class TopK&lt;T&gt; &#123; /** * 堆的边界，Top K 问题中的 K */ private final int boundary; /** * 优先队列，用来构造一个有界的堆 */ private final PriorityQueue&lt;T&gt; boundaryHeap; /** * 通过自定义边界 boundary 可以求解 top K 问题 * 通过自定义比较器 comparator 可以控制求解 top K 大 还是 top K 小 * @param boundary 边界 K * @param comparator 数据比较器 */ public TopK(int boundary, Comparator&lt;T&gt; comparator) &#123; this.boundary = boundary; boundaryHeap = new PriorityQueue&lt;&gt;(boundary, comparator); &#125; /** * 求解数据流中的top K， 将结果写入List中 * @param dataStream 数据流 * @param results top K 结果 */ public void topK(Stream&lt;T&gt; dataStream, List&lt;T&gt; results) &#123; dataStream.forEach(this::add); while (!boundaryHeap.isEmpty()) &#123; results.add(boundaryHeap.poll()); &#125; &#125; /** * 向有界堆中添加元素的帮助方法 * @param t 待添加数据 */ private void add(T t) &#123; boundaryHeap.add(t); if (boundaryHeap.size() &gt; boundary) &#123; boundaryHeap.poll(); &#125; &#125;&#125; 0x02 测试直接写一个main函数进行测试 12345678910111213141516171819202122public static void main(String[] args) &#123; // 构造一个 范围为 [0, 2^30] 的 Integer 流，通过limit可以控制大小 final int upLimit = 1 &lt;&lt; 30; Stream&lt;Integer&gt; stream = Stream.generate(Math::random) .map(d -&gt; d * upLimit) .map(d -&gt; (int) Math.round(d)) .limit(100_000_000); // 将 (o1, o2) -&gt; (o1 - o2) 换成 (o1, o2) -&gt; (o2 - o1) 可以求解 top K 小 TopK&lt;Integer&gt; topK = new TopK&lt;&gt;(10, (o1, o2) -&gt; (o1 - o2)); List&lt;Integer&gt; results = new ArrayList&lt;&gt;(); long startTime = System.currentTimeMillis(); topK.topK(stream, results); long endTime = System.currentTimeMillis(); System.out.println("results: " + results); System.out.println("cost: " + (endTime - startTime) / 1000.0);&#125; 1亿数据测试结果： results: [1073741717, 1073741721, 1073741740, 1073741747, 1073741768, 1073741781, 1073741785, 1073741791, 1073741792, 1073741813]cost: 7.656 0x03 优点分析在输入数据流是一个惰性流(不需要一次性将全部数据加载到内存)的情况下，这种方式速度较快且占用最少的内存，内存中只需要维护一个固定大小的堆即可]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot-mybatis-docker整合使用]]></title>
    <url>%2Fblog%2F2017%2F07%2F23%2Fspring-boot-mybatis-docker%2F</url>
    <content type="text"><![CDATA[0x00 前置条件 Java Maven Docker, Docker Compose 0x01 使用maven新建Spring Boot工程按工程根目录的相对路径创建如下文件 pom.xml12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.tomoyadeng&lt;/groupId&gt; &lt;artifactId&gt;demo-springboot-docker&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; src/main/java/demo/Application.java12345678910111213141516171819package demo;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@SpringBootApplication@RestControllerpublic class Application &#123; @RequestMapping("/") public String home() &#123; return "Get started"; &#125; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 这样，一个简单的Spring boot的应用就创建OK。可使用mvn package编译打包为jar， 然后使用命令行java -jar target/demo-springboot-docker-1.0.0.jar直接启动 0x02 将应用docker化首先创建应用的Dockerfile src/main/docker/Dockerfile123456FROM java:8VOLUME /tmpADD demo-springboot-docker-1.0.0.jar app.jarRUN sh -c 'touch /app.jar'ENV JAVA_OPTS=""ENTRYPOINT [ "sh", "-c", "java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar" ] 然后在pom.xml中添加maven插件依赖，以支持构建docker镜像 pom.xml 12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.11&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;tomoyadeng/$&#123;project.artifactId&#125;&lt;/imageName&gt; &lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt;&lt;/plugin&gt; 使用mvn install docker:build即可构建docker镜像，构建完成后，docker images可查看当前的镜像。 docker run -p 8080:8080 -t tomoyadeng/demo-springboot-docker 可以启动docker容器，此时就完成了此应用的docker化1234567891011tomoya@ubuntu:~/Code/demo-springboot-docker$ docker run -p 8080:8080 -t tomoyadeng/demo-springboot-docker . ____ _ __ _ _ /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.5.4.RELEASE)2017-07-23 11:53:34.894 INFO 5 --- [ main] demo.Application : Starting Application v1.0.0 on f1ff304f4b94 with PID 5 (/app.jar started by root in /)... 可使用docker stop和docker rm来停止容器运行 12345tomoya@ubuntu:~/Code/demo-springboot-docker$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf1ff304f4b94 tomoyadeng/demo-springboot-docker "sh -c 'java $JAVA..." About a minute ago Up About a minute 0.0.0.0:8080-&gt;8080/tcp keen_swartz$ docker stop f1ff304f4b94$ docker rm f1ff304f4b94 0x03 创建MyBatis的demo首先，在pom.xml中添加mybatis和mysql-connector的依赖 pom.xml12345678910&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;6.0.6&lt;/version&gt;&lt;/dependency&gt; 添加object类，此处省略了getter和setter src/main/java/demo/domain/User.java123456789101112131415package demo.domain;import java.io.Serializable;public class User implements Serializable &#123; private static final long serialVersionUID = 1L; private int id; private String name; private String phoneNo; private String email;&#125; 添加mapper src/main/java/demo/mapper/UserMapper.java123456789101112package demo.mapper;import demo.domain.User;import org.apache.ibatis.annotations.Mapper;import org.apache.ibatis.annotations.Param;import org.apache.ibatis.annotations.Select;@Mapperpublic interface UserMapper &#123; @Select("select * from tbl_user where name = #&#123;name&#125;") User findByName(@Param("name") String name);&#125; 修改Application.java，添加查询数据库的操作 src/main/java/demo/Application.java123456789101112131415161718192021222324252627282930313233343536package demo;import demo.domain.User;import demo.mapper.UserMapper;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import static org.springframework.web.bind.annotation.RequestMethod.GET;@SpringBootApplication@RestControllerpublic class Application &#123; final private UserMapper userMapper; public Application(UserMapper userMapper) &#123; this.userMapper = userMapper; &#125; @RequestMapping("/") public String home() &#123; return "Get started"; &#125; @RequestMapping(value = "/user", method = GET) public String getUserByName(@RequestParam("name") String name) &#123; User user = this.userMapper.findByName(name); return user == null ? "No such user!" : user.toString(); &#125; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 添加application.yml配置文件 src/main/resources/application.yml1234567891011121314151617# application.ymlspring: datasource: url: jdbc:mysql://localhost:3306/mybatis?useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;useSSL=false driverClassName: com.mysql.cj.jdbc.Driver username: root password: 123456 schema: classpath:schema.sql---spring: profiles: container datasource: url: jdbc:mysql://$&#123;DATABASE_HOST&#125;:$&#123;DATABASE_PORT&#125;/$&#123;DATABASE_NAME&#125;?useUnicode=true&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;useSSL=false username: $&#123;DATABASE_USER&#125; password: $&#123;DATABASE_PASSWORD&#125; schema: classpath:schema.sql initialize: true 附：schema.sql src/main/resources/schema.sql12345drop table if exists tbl_user;create table tbl_user(id int primary key auto_increment,name varchar(32),phoneNo varchar(16), email varchar(32));insert into tbl_user(name, phoneNo, email) values ('dave', '13012345678', 'dave@tomoyadeng.com'); 修改Dockerfile，主要是修改ENTRYPOINT src/main/docker/Dockerfile123456FROM java:8VOLUME /tmpADD demo-springboot-docker-1.0.0.jar app.jarRUN sh -c 'touch /app.jar'ENV JAVA_OPTS=""ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-Dspring.profiles.active=container","-jar","/app.jar"] 0x04 手动启动docker应用首先，我们需要先启动一个mysql的容器，执行下面命令即可 1234567docker run -d \ --name mybatis-mysql \ -e MYSQL_ROOT_PASSWORD=123456 \ -e MYSQL_DATABASE=mybatis \ -e MYSQL_USER=dbuser \ -e MYSQL_PASSWORD=123456 \ mysql:latest 启动完成后，可用docker ps查看，也可以通过执行下面命令连接到mysql 12docker run -it --link mybatis-mysql:mysql --rm mysql sh \ -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"' 然后，启动应用容器并连接到mysql 12345678910docker run -d -t \ --name demo-springboot-docker \ --link mybatis-mysql:mysql \ -p 8088:8080 \ -e DATABASE_HOST=mybatis-mysql \ -e DATABASE_PORT=3306 \ -e DATABASE_NAME=mybatis \ -e DATABASE_USER=root \ -e DATABASE_PASSWORD=123456 \ tomoyadeng/demo-springboot-docker 启动完成后，使用docker ps查看，或者直接访问url测试 1curl http://localhost:8088/user?name=dave 0x05 使用docker-compose启动在项目根路径下增加docker-compose的配置文件 docker-compose.yml 12345678910111213141516171819202122version: '3.3'services: mybatis-mysql: image: mysql:latest environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=mybatis - MYSQL_USER=dbuser - MYSQL_PASSWORD=123456 demo-springboot-docker: image: tomoyadeng/demo-springboot-docker depends_on: - mybatis-mysql ports: - 8088:8080 environment: - DATABASE_HOST=mybatis-mysql - DATABASE_USER=root - DATABASE_PASSWORD=123456 - DATABASE_NAME=mybatis - DATABASE_PORT=3306 启动前，先将之前手动启动的容器停掉 1234docker stop demo-springboot-dockerdocker stop mybatis-mysqldocker rm demo-springboot-dockerdocker rm mybatis-mysql 然后直接使用命令启动 1docker-compose up]]></content>
      <categories>
        <category>Framework</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
</search>
